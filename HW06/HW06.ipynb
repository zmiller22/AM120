{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AM120 HW05\n",
    "## Zachary Miller\n",
    "### 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4,0,4],\n",
    "              [0,-3,-4],\n",
    "              [8,-3,4],\n",
    "              [20,-6,12]])\n",
    "b = np.array([-4,1,-3,0]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asked to solve the above system using least-squares, which means we want to solve $A^TA\\vec{x}=A^T\\vec{b}$. However, as we can see below, $A^TA$ is has a detirminant of approximately zero and is of rank 2 so there are infinite solutions for $\\vec{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_tA: \n",
      " [[ 480 -144  288]\n",
      " [-144   54  -72]\n",
      " [ 288  -72  192]]\n",
      "Detirminant of A_tA: \n",
      " 1.9645085558295291e-10\n",
      "Rank of A_tA: \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "A_tA = A.T@A\n",
    "print(\"A_tA: \\n\", A_tA)\n",
    "print(\"Detirminant of A_tA: \\n\", np.linalg.det(A_tA))\n",
    "print(\"Rank of A_tA: \\n\", np.linalg.matrix_rank(A_tA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find a unique solution, we have to add an additional constraint to our original least-squares formulation. We now require that the solution $\\vec{x}$ minimizes both the error and the norm of the solution. This can be found using the pseudo inverse to get $\\vec{x}=(A^TA)^{\\dagger}A^T\\vec{b}$ which can be simplifed to $\\vec{x}=A^{\\dagger}\\vec{b}$. Solving below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo Inverse of Sigma:\n",
      " [[0.03785218 0.         0.         0.        ]\n",
      " [0.         0.18878107 0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Pseudo Inverse of A:\n",
      " [[-0.00857843  0.03676471  0.01960784  0.03063725]\n",
      " [ 0.04411765 -0.11764706 -0.02941176 -0.01470588]\n",
      " [ 0.0502451  -0.12009804 -0.01960784  0.01102941]]\n",
      "\n",
      "My Solution:\n",
      " [ 0.0122549  -0.20588235 -0.2622549 ]\n",
      "\n",
      "Solution using np.linalg.lstsq:\n",
      " [ 0.0122549  -0.20588235 -0.2622549 ]\n"
     ]
    }
   ],
   "source": [
    "# Get the SVD of A\n",
    "U, Sigma, V_t = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "# Make the Sigma array from the list of singular values\n",
    "Sigma_diag = np.diag(Sigma)\n",
    "Sigma_mat = np.zeros(A.shape)\n",
    "Sigma_mat[:Sigma_diag.shape[0],:Sigma_diag.shape[1]] = Sigma_diag\n",
    "\n",
    "# Calculate the pseudo inverse of Sigma_mat\n",
    "Sigma_mat_pinv = np.copy(Sigma_mat)\n",
    "Sigma_mat_pinv[Sigma_mat_pinv<1e-14]=0\n",
    "Sigma_mat_pinv[Sigma_mat_pinv != 0] = 1/Sigma_mat_pinv[Sigma_mat_pinv != 0]\n",
    "Sigma_mat_pinv = Sigma_mat_pinv.T\n",
    "\n",
    "# Calculate the pseudo inverse of A using the formula above\n",
    "A_pinv = V_t.T@Sigma_mat_pinv@U.T\n",
    "my_x = A_pinv@b\n",
    "py_x = np.linalg.lstsq(A,b,rcond=None)[0]\n",
    "\n",
    "print(\"Pseudo Inverse of Sigma:\\n\", Sigma_mat_pinv)\n",
    "print(\"\\nPseudo Inverse of A:\\n\", A_pinv)\n",
    "print(\"\\nMy Solution:\\n\", my_x)\n",
    "print(\"\\nSolution using np.linalg.lstsq:\\n\", py_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing our answer to the one obtained using np.linalg.lstsq, we can see that they are the same. From this, we can infer that when faced with infinite possible solutions, np.linalg.lstsq returns the one with the smallest norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-3 -3  2]\n",
      " [-9 -9  6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[-3,-3,2],[-9,-9,6]])\n",
    "b = np.array([2,4]).T\n",
    "\n",
    "print(\"A:\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at the printout of $A$ above, we can see that the second column is the same as the first, and the third column is $-2/3$ times the first column. Therefore, we can immediatly tell that the columns are all linearly dependent, and the rank of $A$ is 1. This implies that there are two free variables, and thus infinite solutions. Again, in order to find a unique solution for this system of equations, we will use the pseudo inverse of $A$ (which we will again find via SVD) to find $\\vec{x}=A^{\\dagger}\\vec{b}$. Doing so below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo Inverse of Sigma:\n",
      " [[0.06741999 0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Pseudo Inverse of A:\n",
      " [[-0.01363636 -0.04090909]\n",
      " [-0.01363636 -0.04090909]\n",
      " [ 0.00909091  0.02727273]]\n",
      "\n",
      "My Solution:\n",
      " [-0.19090909 -0.19090909  0.12727273]\n",
      "\n",
      "Solution using np.linalg.lstsq:\n",
      " [-0.19090909 -0.19090909  0.12727273]\n"
     ]
    }
   ],
   "source": [
    "# Get the SVD of A\n",
    "U, Sigma, V_t = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "# Make the Sigma array from the list of singular values\n",
    "Sigma_diag = np.diag(Sigma)\n",
    "Sigma_mat = np.zeros(A.shape)\n",
    "Sigma_mat[:Sigma_diag.shape[0],:Sigma_diag.shape[1]] = Sigma_diag\n",
    "\n",
    "# Calculate the pseudo inverse of Sigma_mat\n",
    "Sigma_mat_pinv = np.copy(Sigma_mat)\n",
    "Sigma_mat_pinv[Sigma_mat_pinv<1e-14]=0\n",
    "Sigma_mat_pinv[Sigma_mat_pinv != 0] = 1/Sigma_mat_pinv[Sigma_mat_pinv != 0]\n",
    "Sigma_mat_pinv = Sigma_mat_pinv.T\n",
    "\n",
    "# Calculate the pseudo inverse of A using the formula above\n",
    "A_pinv = V_t.T@Sigma_mat_pinv@U.T\n",
    "my_x = A_pinv@b\n",
    "py_x = np.linalg.lstsq(A,b,rcond=None)[0]\n",
    "\n",
    "print(\"Pseudo Inverse of Sigma:\\n\", Sigma_mat_pinv)\n",
    "print(\"\\nPseudo Inverse of A:\\n\", A_pinv)\n",
    "print(\"\\nMy Solution:\\n\", my_x)\n",
    "print(\"\\nSolution using np.linalg.lstsq:\\n\", py_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[ 6, 8, 6],\n",
    "              [-2, 4, 10],\n",
    "              [0, -3, 2],\n",
    "              [4, 3, 2],\n",
    "              [6, -0, 7]])\n",
    "b = np.array([5,-4,-8,2,-2]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to the above overdetirmined system $A\\vec{x}=\\vec{b}$ is the $\\vec{x}$ that minimizes the norm of the error $\\vec{e}=||A\\vec{x}-\\vec{b}||^2$, which is given by $\\vec{e}^T\\vec{e}$. To do this, we take the derivative with respect to $\\vec{x}$ of $\\vec{e}^T\\vec{e}$ and set it to zero. After some calculus, this comes out to be \n",
    "\n",
    "$$2(A^TA\\vec{x}-A^T\\vec{b})=0$$ $$A^TA\\vec{x}=A^T\\vec{b}$$ $$\\vec{x}=(A^TA)^{-1}A^T\\vec{b}$$ \n",
    "\n",
    "Calculating below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least squares solution:\n",
      " [ 0.32153484  1.09494635 -0.79573356]\n"
     ]
    }
   ],
   "source": [
    "x_sol = np.linalg.inv(A.T@A)@A.T@b\n",
    "print(\"Least squares solution:\\n\", x_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the proccess we learned prvioiusly for doing PCA was to first calculate the normalized covariance matrix $C = FF^T/N$, and then the principal components of F were the eigenvectors of C. When doing PCA via SVD, we first calculate $F = U\\Sigma V^T$. Recall that to calculate U, we calculate the normalized eigenvectors of $FF^T$, which is the same as the eigenvectors of $C$. Therefore, we can see that the columns of U from SVD are the same as the PCs of F. \n",
    "\n",
    "When the covariance matrix was used to calculate the PCs, we obtaine the time series $T$ by calculating $T=U^TF$. If we plug in the SVD composition of F for F, we get\n",
    "$$T = U^T(U\\Sigma V^T)$$\n",
    "$$T = \\Sigma V^T$$\n",
    "\n",
    "Finally, since the singluar values along the diagonal of $\\Sigma$ are the square roots of the eigenvalues cooresponding to the eigenvectors that make up the columns of $U$, we can also get back varaince explained for each PC. \n",
    "\n",
    "Therefore, we can see that from the SVD of a data matrix $F$ ,we can get the PCs, their explained variance, and the PC time series. We can also see that the method of doing PCA using SVD will be the same as if we did it using the covariance matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.array([[0,-18,2.5,-10,5,-2.5,7.5,5,10],\n",
    "             [10,25,2.5,12,-5,0,-12,-12,-20],\n",
    "             [-30,12,-20,12,-10,12,0,12,10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asked to calculat the PCs and expansion foefficients using both SVD and the corvariance matrix methods. First let's do this via SVD using the method described in part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCs (columns) using SVD:\n",
      " [[-0.45308868 -0.30380604  0.83810055]\n",
      " [ 0.83700581  0.17858356  0.51723224]\n",
      " [-0.30680926  0.9358471   0.17337323]]\n",
      "PCs (columns) using Covariance Matrix:\n",
      " [[-0.45308868 -0.30380604 -0.83810055]\n",
      " [ 0.83700581  0.17858356 -0.51723224]\n",
      " [-0.30680926  0.9358471  -0.17337323]]\n",
      "\n",
      "Expansion Coefficients using SVD:\n",
      " [[ 1.75743358e+01  2.53990304e+01  7.09597798e+00  1.08932454e+01\n",
      "  -3.38237988e+00 -2.54898939e+00 -1.34422348e+01 -1.59912242e+01\n",
      "  -2.43390956e+01]\n",
      " [-2.62895775e+01  2.11632630e+01 -1.90299983e+01  1.64112284e+01\n",
      "  -1.17704190e+01  1.19896803e+01 -4.42154802e+00  7.56813232e+00\n",
      "   2.74873943e+00]\n",
      " [-2.88746561e-02 -7.45252743e-02 -7.91327010e-02 -9.37399063e-02\n",
      "  -1.29390746e-01 -1.47725798e-02  7.89673266e-02  6.41947468e-02\n",
      "  -2.29906836e-01]]\n",
      "Expansion Coefficients using Covariance Matrix:\n",
      " [[ 1.75743358e+01  2.53990304e+01  7.09597798e+00  1.08932454e+01\n",
      "  -3.38237988e+00 -2.54898939e+00 -1.34422348e+01 -1.59912242e+01\n",
      "  -2.43390956e+01]\n",
      " [-2.62895775e+01  2.11632630e+01 -1.90299983e+01  1.64112284e+01\n",
      "  -1.17704190e+01  1.19896803e+01 -4.42154802e+00  7.56813232e+00\n",
      "   2.74873943e+00]\n",
      " [ 2.88746561e-02  7.45252743e-02  7.91327010e-02  9.37399063e-02\n",
      "   1.29390746e-01  1.47725798e-02 -7.89673266e-02 -6.41947468e-02\n",
      "   2.29906836e-01]]\n"
     ]
    }
   ],
   "source": [
    "### First do PCA using SVD ###\n",
    "\n",
    "# Get the SVD of A\n",
    "U_svd, Sigma, V_t = np.linalg.svd(F, full_matrices=True)\n",
    "\n",
    "# Get list of singular values into matrix form\n",
    "Sigma_diag = np.diag(Sigma)\n",
    "Sigma_mat = np.zeros(F.shape)\n",
    "Sigma_mat[:Sigma_diag.shape[0],:Sigma_diag.shape[1]] = Sigma_diag\n",
    "\n",
    "# Calculat the expansion coefficients\n",
    "T_svd = Sigma_mat@V_t\n",
    "\n",
    "### Now do PCA using covariance matrix ###\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "C = F@F.T/F.shape[1]\n",
    "\n",
    "# Calculate the eigenvectors and eigenvalues of F\n",
    "eigvals, eigvecs = np.linalg.eig(C)\n",
    "\n",
    "# Sort the eigenvectors so that the eigenvectors cooresponding to the largest eigenvalues\n",
    "# are first\n",
    "inds = (-np.abs(eigvals)).argsort()\n",
    "U_cov = eigvecs [:,inds]\n",
    "eigvals = eigvals[inds]\n",
    "\n",
    "# Calculate the expansion coefficients\n",
    "T_cov = U_cov.T@F\n",
    "\n",
    "### Compare Results ###\n",
    "print(\"PCs (columns) using SVD:\\n\", U_svd)\n",
    "print(\"PCs (columns) using Covariance Matrix:\\n\", U_cov)\n",
    "print(\"\\nExpansion Coefficients using SVD:\\n\", T_svd)\n",
    "print(\"Expansion Coefficients using Covariance Matrix:\\n\", T_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above, we can see that the numbers are all the same, save for some differences in sign. As we have seen in previous problems, since eigenvectors are only uniquely detirmined up to a minus sign, principal components may come out with different signs depending on how they are calculated. Importantly, however, the interpretation of each principal component remains the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45308868, -0.30380604, -0.83810055],\n",
       "       [ 0.83700581,  0.17858356, -0.51723224],\n",
       "       [-0.30680926,  0.9358471 , -0.17337323]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45308868, -0.30380604,  0.83810055],\n",
       "       [ 0.83700581,  0.17858356,  0.51723224],\n",
       "       [-0.30680926,  0.9358471 ,  0.17337323]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
