{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AM120 HW05\n",
    "## Zachary Miller\n",
    "### 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4,0,4],\n",
    "              [0,-3,-4],\n",
    "              [8,-3,4],\n",
    "              [20,-6,12]])\n",
    "b = np.array([-4,1,-3,0]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asked to solve the above system using least-squares, which means we want to solve $A^TA\\vec{x}=A^T\\vec{b}$. However, as we can see below, $A^TA$ is has a detirminant of approximately zero and is of rank 2 so there are infinite solutions for $\\vec{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_tA: \n",
      " [[ 480 -144  288]\n",
      " [-144   54  -72]\n",
      " [ 288  -72  192]]\n",
      "Detirminant of A_tA: \n",
      " 1.9645085558295291e-10\n",
      "Rank of A_tA: \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "A_tA = A.T@A\n",
    "print(\"A_tA: \\n\", A_tA)\n",
    "print(\"Detirminant of A_tA: \\n\", np.linalg.det(A_tA))\n",
    "print(\"Rank of A_tA: \\n\", np.linalg.matrix_rank(A_tA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find a unique solution, we have to add an additional constraint to our original least-squares formulation. We now require that the solution $\\vec{x}$ minimizes both the error and the norm of the solution. This can be found using the pseudo inverse to get $\\vec{x}=(A^TA)^{\\dagger}A^T\\vec{b}$ which can be simplifed to $\\vec{x}=A^{\\dagger}\\vec{b}$. Solving below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo Inverse of Sigma:\n",
      " [[0.03785218 0.         0.         0.        ]\n",
      " [0.         0.18878107 0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Pseudo Inverse of A:\n",
      " [[-0.00857843  0.03676471  0.01960784  0.03063725]\n",
      " [ 0.04411765 -0.11764706 -0.02941176 -0.01470588]\n",
      " [ 0.0502451  -0.12009804 -0.01960784  0.01102941]]\n",
      "\n",
      "My Solution:\n",
      " [ 0.0122549  -0.20588235 -0.2622549 ]\n",
      "\n",
      "Solution using np.linalg.lstsq:\n",
      " [ 0.0122549  -0.20588235 -0.2622549 ]\n"
     ]
    }
   ],
   "source": [
    "# Get the SVD of A\n",
    "U, Sigma, V_t = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "# Make the Sigma array from the list of singular values\n",
    "Sigma_diag = np.diag(Sigma)\n",
    "Sigma_mat = np.zeros(A.shape)\n",
    "Sigma_mat[:Sigma_diag.shape[0],:Sigma_diag.shape[1]] = Sigma_diag\n",
    "\n",
    "# Calculate the pseudo inverse of Sigma_mat\n",
    "Sigma_mat_pinv = np.copy(Sigma_mat)\n",
    "Sigma_mat_pinv[Sigma_mat_pinv<1e-14]=0\n",
    "Sigma_mat_pinv[Sigma_mat_pinv != 0] = 1/Sigma_mat_pinv[Sigma_mat_pinv != 0]\n",
    "Sigma_mat_pinv = Sigma_mat_pinv.T\n",
    "\n",
    "# Calculate the pseudo inverse of A using the formula above\n",
    "A_pinv = V_t.T@Sigma_mat_pinv@U.T\n",
    "my_x = A_pinv@b\n",
    "py_x = np.linalg.lstsq(A,b,rcond=None)[0]\n",
    "\n",
    "print(\"Pseudo Inverse of Sigma:\\n\", Sigma_mat_pinv)\n",
    "print(\"\\nPseudo Inverse of A:\\n\", A_pinv)\n",
    "print(\"\\nMy Solution:\\n\", my_x)\n",
    "print(\"\\nSolution using np.linalg.lstsq:\\n\", py_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing our answer to the one obtained using np.linalg.lstsq, we can see that they are the same. From this, we can infer that when faced with infinite possible solutions, np.linalg.lstsq returns the one with the smallest norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-3 -3  2]\n",
      " [-9 -9  6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[-3,-3,2],[-9,-9,6]])\n",
    "b = np.array([2,4]).T\n",
    "\n",
    "print(\"A:\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at the printout of $A$ above, we can see that the second column is the same as the first, and the third column is $-2/3$ times the first column. Therefore, we can immediatly tell that the columns are all linearly dependent, and the rank of $A$ is 1. This implies that there are two free variables, and thus infinite solutions. Again, in order to find a unique solution for this system of equations, we will use the pseudo inverse of $A$ (which we will again find via SVD) to find $\\vec{x}=A^{\\dagger}\\vec{b}$. Doing so below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo Inverse of Sigma:\n",
      " [[0.06741999 0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Pseudo Inverse of A:\n",
      " [[-0.01363636 -0.04090909]\n",
      " [-0.01363636 -0.04090909]\n",
      " [ 0.00909091  0.02727273]]\n",
      "\n",
      "My Solution:\n",
      " [-0.19090909 -0.19090909  0.12727273]\n",
      "\n",
      "Solution using np.linalg.lstsq:\n",
      " [-0.19090909 -0.19090909  0.12727273]\n"
     ]
    }
   ],
   "source": [
    "# Get the SVD of A\n",
    "U, Sigma, V_t = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "# Make the Sigma array from the list of singular values\n",
    "Sigma_diag = np.diag(Sigma)\n",
    "Sigma_mat = np.zeros(A.shape)\n",
    "Sigma_mat[:Sigma_diag.shape[0],:Sigma_diag.shape[1]] = Sigma_diag\n",
    "\n",
    "# Calculate the pseudo inverse of Sigma_mat\n",
    "Sigma_mat_pinv = np.copy(Sigma_mat)\n",
    "Sigma_mat_pinv[Sigma_mat_pinv<1e-14]=0\n",
    "Sigma_mat_pinv[Sigma_mat_pinv != 0] = 1/Sigma_mat_pinv[Sigma_mat_pinv != 0]\n",
    "Sigma_mat_pinv = Sigma_mat_pinv.T\n",
    "\n",
    "# Calculate the pseudo inverse of A using the formula above\n",
    "A_pinv = V_t.T@Sigma_mat_pinv@U.T\n",
    "my_x = A_pinv@b\n",
    "py_x = np.linalg.lstsq(A,b,rcond=None)[0]\n",
    "\n",
    "print(\"Pseudo Inverse of Sigma:\\n\", Sigma_mat_pinv)\n",
    "print(\"\\nPseudo Inverse of A:\\n\", A_pinv)\n",
    "print(\"\\nMy Solution:\\n\", my_x)\n",
    "print(\"\\nSolution using np.linalg.lstsq:\\n\", py_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[ 6, 8, 6],\n",
    "              [-2, 4, 10],\n",
    "              [0, -3, 2],\n",
    "              [4, 3, 2],\n",
    "              [6, -0, 7]])\n",
    "b = np.array([5,-4,-8,2,-2]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to the above overdetirmined system $A\\vec{x}=\\vec{b}$ is the $\\vec{x}$ that minimizes the norm of the error $\\vec{e}=||A\\vec{x}-\\vec{b}||^2$, which is given by $\\vec{e}^T\\vec{e}$. To do this, we take the derivative with respect to $\\vec{x}$ of $\\vec{e}^T\\vec{e}$ and set it to zero. After some calculus, this comes out to be \n",
    "\n",
    "$$2(A^TA\\vec{x}-A^T\\vec{b})=0$$ $$A^TA\\vec{x}=A^T\\vec{b}$$ $$\\vec{x}=(A^TA)^{-1}A^T\\vec{b}$$ \n",
    "\n",
    "Calculating below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least squares solution:\n",
      " [ 0.32153484  1.09494635 -0.79573356]\n"
     ]
    }
   ],
   "source": [
    "x_sol = np.linalg.inv(A.T@A)@A.T@b\n",
    "print(\"Least squares solution:\\n\", x_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are asked to find the economical QR decomposition of A. To do this, we first apply the Gram_Schmidt proccss to the columns of $A$ to find $Q$. We will then use the fact that $A=QR \\implies Q^{-1}A=R$ to find $R$. Doing this below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " [[ 6.          4.60869565 -1.70975919]\n",
      " [-2.          5.13043478  7.64385298]\n",
      " [ 0.         -3.          4.21673004]\n",
      " [ 4.          0.73913043 -1.4157161 ]\n",
      " [ 6.         -3.39130435  5.20152091]]\n",
      "R:\n",
      " [[ 92.          52.          66.        ]\n",
      " [  0.          68.60869565  50.69565217]\n",
      " [  0.           0.         108.19264892]]\n"
     ]
    }
   ],
   "source": [
    "# Apply Gram-Schmidt to columns of A to find Q\n",
    "q0 = A[:,0]\n",
    "q1 = A[:,1] - (np.dot(A[:,1],q0)/np.dot(q0,q0))*q0\n",
    "q2 = (A[:,2] - (np.dot(A[:,2],q0)/np.dot(q0,q0))*q0 - \n",
    "      (np.dot(A[:,2],q1)/np.dot(q1,q1))*q1)\n",
    "Q = np.array([q0, q1, q2]).T\n",
    "\n",
    "# Q_T to find R\n",
    "R = Q.T@A\n",
    "R[R<1e-14] = 0\n",
    "\n",
    "print(\"Q:\\n\",Q)\n",
    "print(\"R:\\n\",R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c\n",
    "Now, we can use the QR decomposition of A to solve for X, since \n",
    "$$A\\vec{x} = \\vec{b}$$\n",
    "$$QR\\vec{x} = \\vec{b}$$\n",
    "$$R\\vec{x} = Q^T \\vec{b}$$\n",
    "Let $Q^T \\vec{b}$ be $\\vec{b}^*$, then calculating $\\vec{b}^*$ below, we find..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_star = \n",
      " [ 34.          34.7826087  -86.09252218]\n"
     ]
    }
   ],
   "source": [
    "b_star = Q.T@b\n",
    "print(\"b_star = \\n\", b_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily solve the equation $R\\vec{x}=\\vec{b}^*$ using forward and back substitions. The handwritten work is attached at the bottom of this file. The answer comes out to be that $\\vec{x} = [0.322;1.095;-0.796]$ \n",
    "\n",
    "### 2d\n",
    "The answer obtained in part c is the same as the answer from part a (within round off error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the proccess we learned prvioiusly for doing PCA was to first calculate the normalized covariance matrix $C = FF^T/N$, and then the principal components of F were the eigenvectors of C. When doing PCA via SVD, we first calculate $F = U\\Sigma V^T$. Recall that to calculate U, we calculate the normalized eigenvectors of $FF^T$, which is the same as the eigenvectors of $C$. Therefore, we can see that the columns of U from SVD are the same as the PCs of F. \n",
    "\n",
    "When the covariance matrix was used to calculate the PCs, we obtaine the time series $T$ by calculating $T=U^TF$. If we plug in the SVD composition of F for F, we get\n",
    "$$T = U^T(U\\Sigma V^T)$$\n",
    "$$T = \\Sigma V^T$$\n",
    "\n",
    "Finally, since the singluar values along the diagonal of $\\Sigma$ are the square roots of the eigenvalues cooresponding to the eigenvectors that make up the columns of $U$, we can also get back varaince explained for each PC. \n",
    "\n",
    "Therefore, we can see that from the SVD of a data matrix $F$ ,we can get the PCs, their explained variance, and the PC time series. We can also see that the method of doing PCA using SVD will be the same as if we did it using the covariance matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.array([[0,-18,2.5,-10,5,-2.5,7.5,5,10],\n",
    "             [10,25,2.5,12,-5,0,-12,-12,-20],\n",
    "             [-30,12,-20,12,-10,12,0,12,10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asked to calculat the PCs and expansion foefficients using both SVD and the corvariance matrix methods. First let's do this via SVD using the method described in part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCs (columns) using SVD:\n",
      " [[-0.45308868 -0.30380604  0.83810055]\n",
      " [ 0.83700581  0.17858356  0.51723224]\n",
      " [-0.30680926  0.9358471   0.17337323]]\n",
      "PCs (columns) using Covariance Matrix:\n",
      " [[-0.45308868 -0.30380604 -0.83810055]\n",
      " [ 0.83700581  0.17858356 -0.51723224]\n",
      " [-0.30680926  0.9358471  -0.17337323]]\n",
      "\n",
      "Expansion Coefficients using SVD:\n",
      " [[ 1.75743358e+01  2.53990304e+01  7.09597798e+00  1.08932454e+01\n",
      "  -3.38237988e+00 -2.54898939e+00 -1.34422348e+01 -1.59912242e+01\n",
      "  -2.43390956e+01]\n",
      " [-2.62895775e+01  2.11632630e+01 -1.90299983e+01  1.64112284e+01\n",
      "  -1.17704190e+01  1.19896803e+01 -4.42154802e+00  7.56813232e+00\n",
      "   2.74873943e+00]\n",
      " [-2.88746561e-02 -7.45252743e-02 -7.91327010e-02 -9.37399063e-02\n",
      "  -1.29390746e-01 -1.47725798e-02  7.89673266e-02  6.41947468e-02\n",
      "  -2.29906836e-01]]\n",
      "Expansion Coefficients using Covariance Matrix:\n",
      " [[ 1.75743358e+01  2.53990304e+01  7.09597798e+00  1.08932454e+01\n",
      "  -3.38237988e+00 -2.54898939e+00 -1.34422348e+01 -1.59912242e+01\n",
      "  -2.43390956e+01]\n",
      " [-2.62895775e+01  2.11632630e+01 -1.90299983e+01  1.64112284e+01\n",
      "  -1.17704190e+01  1.19896803e+01 -4.42154802e+00  7.56813232e+00\n",
      "   2.74873943e+00]\n",
      " [ 2.88746561e-02  7.45252743e-02  7.91327010e-02  9.37399063e-02\n",
      "   1.29390746e-01  1.47725798e-02 -7.89673266e-02 -6.41947468e-02\n",
      "   2.29906836e-01]]\n"
     ]
    }
   ],
   "source": [
    "### First do PCA using SVD ###\n",
    "\n",
    "# Get the SVD of A\n",
    "U_svd, Sigma, V_t = np.linalg.svd(F, full_matrices=True)\n",
    "\n",
    "# Get list of singular values into matrix form\n",
    "Sigma_diag = np.diag(Sigma)\n",
    "Sigma_mat = np.zeros(F.shape)\n",
    "Sigma_mat[:Sigma_diag.shape[0],:Sigma_diag.shape[1]] = Sigma_diag\n",
    "\n",
    "# Calculat the expansion coefficients\n",
    "T_svd = Sigma_mat@V_t\n",
    "\n",
    "### Now do PCA using covariance matrix ###\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "C = F@F.T/F.shape[1]\n",
    "\n",
    "# Calculate the eigenvectors and eigenvalues of F\n",
    "eigvals, eigvecs = np.linalg.eig(C)\n",
    "\n",
    "# Sort the eigenvectors so that the eigenvectors cooresponding to the largest eigenvalues\n",
    "# are first\n",
    "inds = (-np.abs(eigvals)).argsort()\n",
    "U_cov = eigvecs [:,inds]\n",
    "eigvals = eigvals[inds]\n",
    "\n",
    "# Calculate the expansion coefficients\n",
    "T_cov = U_cov.T@F\n",
    "\n",
    "### Compare Results ###\n",
    "print(\"PCs (columns) using SVD:\\n\", U_svd)\n",
    "print(\"PCs (columns) using Covariance Matrix:\\n\", U_cov)\n",
    "print(\"\\nExpansion Coefficients using SVD:\\n\", T_svd)\n",
    "print(\"Expansion Coefficients using Covariance Matrix:\\n\", T_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above, we can see that the numbers are all the same, save for some differences in sign. As we have seen in previous problems, since eigenvectors are only uniquely detirmined up to a minus sign, principal components may come out with different signs depending on how they are calculated. Importantly, however, the interpretation of each principal component remains the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[4,2.6,2.2,3.8,3.4,2,3,4,2.6,2.2,3.8],\n",
    "             [1.9,3.3,3.9,2.2,2.7,4.1,3.1,2.1,3.3,3.8,2.3],\n",
    "             [-1,4.6,6.2,-0.1,1.4,7,3.1,-0.9,4.6,6.2,-0.1],\n",
    "             [-0.8,4.9,6,-0.2,1.1,6.7,2.9,-0.9,4.9,6.1,-0.2]])\n",
    "Y = np.array([[-2.6,-5.9,-7.3,-3.1,-4,-7.7,-4.9,-2.4,-6.1,-7.2,-2.9],\n",
    "             [-7.7,-4,-2.5,-7.2,-6.2,-2,-5.1,-7.9,-3.8,-2.5,-7.4],\n",
    "             [-7.3,-4,-3.1,-6.9,-6,-2.7,-5,-7.3,-4,-3.1,-6.9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform multivariate PCA on the above data, we must first remove subtract the mean of each row. Then, we need to normalize each value in X by the standard deviation of all the X values, and then do the same seperately for the Y data. Note that this just means we want to divide each entry in X by the standard deviation of all entries i X, and likewise for Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized X Data:\n",
      " [[ 4.   2.6  2.2  3.8  3.4  2.   3.   4.   2.6  2.2  3.8]\n",
      " [ 1.9  3.3  3.9  2.2  2.7  4.1  3.1  2.1  3.3  3.8  2.3]\n",
      " [-1.   4.6  6.2 -0.1  1.4  7.   3.1 -0.9  4.6  6.2 -0.1]\n",
      " [-0.8  4.9  6.  -0.2  1.1  6.7  2.9 -0.9  4.9  6.1 -0.2]]\n",
      "Normalized Y Data:\n",
      " [[-2.6 -5.9 -7.3 -3.1 -4.  -7.7 -4.9 -2.4 -6.1 -7.2 -2.9]\n",
      " [-7.7 -4.  -2.5 -7.2 -6.2 -2.  -5.1 -7.9 -3.8 -2.5 -7.4]\n",
      " [-7.3 -4.  -3.1 -6.9 -6.  -2.7 -5.  -7.3 -4.  -3.1 -6.9]]\n"
     ]
    }
   ],
   "source": [
    "# Remove mean from each row\n",
    "X_norm = X-np.mean(X, axis=1).reshape(X.shape[0],1)\n",
    "Y_norm = Y-np.mean(Y, axis=1).reshape(Y.shape[0],1)\n",
    "\n",
    "# Normalize by std of each group\n",
    "X_norm = X_norm/np.std(X)\n",
    "Y_norm = Y_norm/np.std(Y)\n",
    "\n",
    "print(\"Normalized X Data:\\n\", X)\n",
    "print(\"Normalized Y Data:\\n\", Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9978911409180963\n",
      "PCs (columns):\n",
      " [[-0.13142226 -0.02092185 -0.09924493 -0.09320572 -0.01404402  0.33223223 -0.92367641]\n",
      " [ 0.13111547  0.28772149  0.53764473 -0.48628455 -0.45008344  0.39802939  0.11613808]\n",
      " [ 0.5204419   0.09070272  0.57052637  0.50443552  0.3268824   0.03136549 -0.18199399]\n",
      " [ 0.51557648 -0.74527634 -0.18488136 -0.0489759  -0.19951146  0.30910502  0.08254438]\n",
      " [-0.3735996  -0.42537059  0.38875174 -0.34856701  0.61339404  0.14437205  0.09879669]\n",
      " [ 0.41941733  0.40403955 -0.42205465 -0.34736188  0.52375119  0.2775528   0.10344021]\n",
      " [ 0.33670533 -0.09448128  0.11022293 -0.5057934  -0.00177971 -0.73265667 -0.27006995]]\n",
      "\n",
      "Number of PCs needed to explain at least 50% of total variance: 1\n",
      "Variance explained by 1 PCs: 0.9978911409180963\n"
     ]
    }
   ],
   "source": [
    "# Combine the normalized X and Y data into one dataset\n",
    "F = np.vstack((X_norm,Y_norm))\n",
    "\n",
    "# Get the SVD of F\n",
    "U, Sigma, V_t = np.linalg.svd(F, full_matrices=True)\n",
    "\n",
    "# Make the Sigma array from the list of singular values\n",
    "Sigma_diag = np.diag(Sigma)\n",
    "Sigma_mat = np.zeros(F.shape)\n",
    "Sigma_mat[:Sigma_diag.shape[0],:Sigma_diag.shape[1]] = Sigma_diag\n",
    "\n",
    "#print(\"PCs (columns):\\n\", U)\n",
    "\n",
    "total_var = np.sum([np.square(i) for i in Sigma])\n",
    "running_sum = 0\n",
    "num_comps = -1\n",
    "for i, val in enumerate(Sigma):\n",
    "    running_sum += np.square(val)\n",
    "    print(running_sum/total_var)\n",
    "    if running_sum/total_var >= 0.5:\n",
    "        num_comps = i+1\n",
    "        break\n",
    "\n",
    "print(\"PCs (columns):\\n\", U)\n",
    "print(\"\\nNumber of PCs needed to explain at least 50% of total variance:\", num_comps)\n",
    "print(\"Variance explained by\", num_comps, \"PCs:\", running_sum/total_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above, we can see that almost all of the variance in the dataset is contained within the first PC. Looking at this first PC, we can see that the first and fifth elements are negative, while the rest are all positive. Therefore, we can conclude that the prices in the first and fifth products vary together in one direction, where as the prices of all the other products vary together in the other direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c\n",
    "We know that PCA requires each PC to be orthogonal, and for this reason can sometimes miss relationships in the data. Therefore, it is also prudent to investigate the covariance between the two datasets in case PCA was dominated by large variances between the datasets that consequently obscured our ability to identify the presence smaller covariance between the datasets. We can check this by looking at the covariance matrix between $X$ and $Y$, given by $C=XY^T/N$ after the mean has been removed from the rows of X and Y. Looking at this below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\n",
      " [[ 1.42280992 -1.59719008 -1.28264463]\n",
      " [-1.42140496  1.59495868  1.27950413]\n",
      " [-5.63330579  6.32396694  5.07942149]\n",
      " [-5.57049587  6.25586777  5.03404959]]\n",
      "\n",
      "Total covariance: 205.45717312342055\n"
     ]
    }
   ],
   "source": [
    "X_nm = (X-np.mean(X, axis=1).reshape(X.shape[0],1))\n",
    "Y_nm = (Y-np.mean(Y, axis=1).reshape(Y.shape[0],1))\n",
    "\n",
    "C = (X_nm@Y_nm.T)/X_nm.shape[1]\n",
    "print(\"C:\\n\", C)\n",
    "print(\"\\nTotal covariance:\", np.sum(np.square(C)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows of $C$ represent the the four products in $X$ while the columns represent the 3 products in $Y$. The elements represent the covariance between the products cooresponding to each row and column. Looking closely at this covariance matrix, we can see the same features that we saw in the first PC of our PCA analysis. We can see that $X_1$ and $Y_1$ are positively coorelated, which we saw from those terms both being negative in the first PC, and we can also see that $X_1$ and $Y_1$ are both negatively coorelated with all the other products (which are in turn all positively coorelated with each other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d\n",
    "Now that we have calculated $C$, we can use SVD on $C$ to perform MCA on this data. Doing this below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values of C:\n",
      " [1.43337757e+01 6.97365562e-03 1.68405371e-04]\n",
      "\n",
      "U:\n",
      " [[-0.17400224  0.17349134  0.09290314  0.96487978]\n",
      " [ 0.17373391 -0.37598545  0.9101229   0.01130403]\n",
      " [ 0.68897311 -0.56646087 -0.36878125  0.26160757]\n",
      " [ 0.68180335  0.71250107  0.16445579 -0.0209932 ]]\n",
      "\n",
      "V:\n",
      " [[-0.57024035  0.47834665 -0.66784012]\n",
      " [ 0.64025893 -0.25055083 -0.72614929]\n",
      " [ 0.51467898  0.84167022  0.16339151]]\n"
     ]
    }
   ],
   "source": [
    "# Get the SVD of C\n",
    "U, Sigma, V_T = np.linalg.svd(C, full_matrices=True)\n",
    "\n",
    "print(\"Singular values of C:\\n\",Sigma)\n",
    "print(\"\\nU:\\n\",U)\n",
    "print(\"\\nV:\\n\",V_T.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the SVD of $C$, we can see that the singular values are dominated by one value which is orders of magnitude greater than the other two. Since these singluar values divided by their sum represent the fraction of the total covariance explained by each SVD mode, we can tell that we only need the first SVD mode to explain most of the covariance between the two datasets. The column vectors of $U$ and $V$ represent the structures in X and Y (respectively) that vary together. Notice that the strucure of the first columbs of $U$ and $V$ above is very similar to that of our first PC. Since we see that almost all of the total covariance is explained by the first SVD mode, and since this has similar structure to the only PC we decided to keep, we can tell that PCA did not miss any significant covariances in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4e\n",
    "We know that the total covariance is equal to the sum of all the singular values of $C$ squared. Checking that this is true below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total squared sum of elements of C: 205.45717312342055\n",
      "Total squared sum of elements of Singular Values of C: 205.45717312342057\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total covariance two different ways\n",
    "total_cov = np.sum(np.square(C))\n",
    "total_cov_sv = np.sum(np.square(Sigma))\n",
    "\n",
    "print(\"Total squared sum of elements of C:\",total_cov)\n",
    "print(\"Total squared sum of elements of Singular Values of C:\",total_cov_sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that the total covariance of $C$ is equal to the sum of squared singular values of $C$. We don't even need to do any calculation to see that only one SVD mode explains well over 50% of the total covariance given our observations in part d (the portion of the covariance explained by the first SVD mode is printed below anyway). In general, the difference between total variance and total covariance is that total variance represents the total deviation from the mean for each variable of the dataset, which can also be thought of as the covariance of that varaible with itself. Total covariance, on the other hand, represents the total joint variability of each possible pair of variables. However, in the case above when we are performing MCA, we only consider the covariance of pairs of $X$ and $Y$ variables, not the covariance of variable pairs within $X$ or $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999997631611869"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(Sigma[0])/np.sum(np.square(Sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jaccard similarity of two sets $A$ and $B$ is defined as\n",
    "$$J(A,B) = \\frac{|A\\cap B|}{|A\\cup B|}$$\n",
    "Below, we calculate the Jaccard similarity of a few different sets\n",
    "\n",
    "#### $\\text{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.125\n"
     ]
    }
   ],
   "source": [
    "# Define the sets\n",
    "a=np.array([1,1,0,0,0,0,0,0,1,1])\n",
    "b=np.array([0,0,1,0,1,1,1,0,1,0])\n",
    "\n",
    "# Find the Jaccard Similarity\n",
    "jac_sim = np.sum(a&b)/np.sum(a|b)\n",
    "print(\"Jaccard Similarity:\", jac_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\text{ii}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.625\n"
     ]
    }
   ],
   "source": [
    "# Define the sets\n",
    "a=set([2, 2, 1, 3, 4, 6, 2, 2, 8, 2])\n",
    "b=set([2, 1, 8, 7, 9, 4, 7, 6, 4, 8])\n",
    "\n",
    "# Find the Jaccard Similarity\n",
    "jac_sim = len(a&b)/len(a|b)\n",
    "print(\"Jaccard Similarity:\", jac_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\text{iii}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.55\n"
     ]
    }
   ],
   "source": [
    "# Define the sets\n",
    "a = set(['what','are','the','roots','that','clutch','what','branches','grow','out','of','this'\n",
    "    ,'stony','rubbish','son','of','man','you','cannot','say','or','guess','for','you','know'\n",
    "    ,'only','a','heap','of','broken','images','where','the','sun','beats','and'])\n",
    "\n",
    "b = set(['cricket','dry','what','no','sound','man','out','know','sun','stone','images','water'\n",
    "    ,'no','what','grow','tree','you','that','cannot','this','guess','say','the','the','of'\n",
    "    ,'the','roots','and','broken','heap','you','gives','only','dead','rubbish','clutch'])\n",
    "\n",
    "# Find the Jaccard Similarity\n",
    "jac_sim = len(a&b)/len(a|b)\n",
    "print(\"Jaccard Similarity:\", jac_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\text{iv}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.16071428571428573\n"
     ]
    }
   ],
   "source": [
    "## Code adapted from website ##\n",
    "# Read in the text files\n",
    "fid1 = open('words1.txt','r');\n",
    "fid2 = open('words2.txt','r');\n",
    "carray1 =[]; carray2 =[]\n",
    "contents = fid1.readlines()\n",
    "for i in range(len(contents)):\n",
    "    carray1.append(re.split(\"[ \\r\\n\\t“”’.,-]+\",contents[i]))\n",
    "fid1.close()\n",
    "contents = fid2.readlines()\n",
    "for i in range(len(contents)):\n",
    "    carray2.append(re.split(\"[ \\r\\n\\t“”’.,-]+\",contents[i]))\n",
    "fid2.close()\n",
    "## words come as a list of sublists, each corresponding to a line.\n",
    "## next, need to \"flatten\" the lists, that is, create a single list of\n",
    "## words for each file:\n",
    "a= set([item for sublist in carray1 for item in sublist])\n",
    "b= set([item for sublist in carray2 for item in sublist])\n",
    "\n",
    "# Find the Jaccard Similarity\n",
    "jac_sim = len(a&b)/len(a|b)\n",
    "print(\"Jaccard Similarity:\", jac_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
