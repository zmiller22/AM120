{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW10 \n",
    "## Zachary Miller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a\n",
    "We are given the below parameters and asked to draw the neural network they represent along with the equations for each layer. This is done by hand below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=np.array([[ 0.336067, -0.322224],[-0.700076, 0.00861628]])\n",
    "b2=np.array([ 4.17819,-6.55492]).reshape(2,1)\n",
    "w3= np.array([[ -106.923, -1936.34],[-36.9631, 1949.76]])    \n",
    "b3=np.array([113.353, 27.4203]).reshape(2,1)\n",
    "w4=np.array([1138.29, 2871.45])\n",
    "b4=np.array([-1138.24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b\n",
    "We are now asked to calculate the outputs for five different possible inputs, as well as for the input $[1.5;1,7]$. We are also asked to show the intermediate neuron values for the last input. Doing this below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Values:\n",
      "[[1.07657994 1.82405421 1.24361556 1.02127347 1.56959945]\n",
      " [1.31792961 1.30440459 1.0474012  1.36628162 1.17881533]]\n",
      "\n",
      "Output for [1.07657994 1.31792961]:\n",
      "[0.22341123]\n",
      "\n",
      "Output for [1.82405421 1.30440459]:\n",
      "[-0.28575606]\n",
      "\n",
      "Output for [1.24361556 1.0474012 ]:\n",
      "[-0.15908463]\n",
      "\n",
      "Output for [1.02127347 1.36628162]:\n",
      "[0.33667149]\n",
      "\n",
      "Output for [1.56959945 1.17881533]:\n",
      "[-0.24985876]\n",
      "\n",
      "[1.5;1.7] input with steps:\n",
      "\n",
      "z2:\n",
      "[[ 4.1345097 ]\n",
      " [-7.59038632]]\n",
      "\n",
      "a2:\n",
      "[[9.84241784e-01]\n",
      " [5.05030627e-04]]\n",
      "\n",
      "z3:\n",
      "[[ 7.13700473]\n",
      " [-7.97563897]]\n",
      "\n",
      "a3:\n",
      "[[9.99205502e-01]\n",
      " [3.43617085e-04]]\n",
      "\n",
      "z4:\n",
      "[0.13230966]\n",
      "\n",
      "a4:\n",
      "[0.13230966]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    sig=1.0/(1.0+np.exp(-x));\n",
    "    return sig\n",
    "\n",
    "# Calculate the output of each layer as a funciton of the first layer\n",
    "# before and after the activation function\n",
    "def calc_output(x, verbose=False):\n",
    "    z2 = w2@x+b2\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = w3@a2+b3\n",
    "    a3 = sigmoid(z3)\n",
    "    z4 = w4@a3+b4\n",
    "    a4 = 1*z4\n",
    "    \n",
    "    if verbose==True:\n",
    "        print(f'z2:\\n{z2}')\n",
    "        print(f'\\na2:\\n{a2}')\n",
    "        print(f'\\nz3:\\n{z3}')\n",
    "        print(f'\\na3:\\n{a3}')\n",
    "        print(f'\\nz4:\\n{z4}')\n",
    "        print(f'\\na4:\\n{a4}')\n",
    "    return a4\n",
    "\n",
    "test_vals = np.random.rand(2,5)+1\n",
    "print(f'Test Values:\\n{test_vals}\\n')\n",
    "\n",
    "outputs = []\n",
    "for i in range(5):\n",
    "    test_val = test_vals[:,i].reshape(2,1)\n",
    "    output = calc_output(test_val)\n",
    "    outputs.append(output)\n",
    "    print(f'Output for {test_vals[:,i]}:\\n{output}\\n')\n",
    "    \n",
    "print(f'[1.5;1.7] input with steps:\\n')\n",
    "output = calc_output(np.array([[1.5],[1.7]]),verbose=True)\n",
    "outputs.append(output)\n",
    "test_vals = np.hstack((test_vals, np.array([[1.5],[1.7]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c\n",
    "Now we are asked to confirm that this network is the same as the function $y=(x_2-x_1)/x_1$. Doing this below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected output for given function:\n",
      "Input:\n",
      "[[1.07657994]\n",
      " [1.31792961]]\n",
      "Function output:\n",
      "[0.22418184]\n",
      "Network output:\n",
      "[0.22341123]\n",
      "\n",
      "Input:\n",
      "[[1.82405421]\n",
      " [1.30440459]]\n",
      "Function output:\n",
      "[-0.28488716]\n",
      "Network output:\n",
      "[-0.28575606]\n",
      "\n",
      "Input:\n",
      "[[1.24361556]\n",
      " [1.0474012 ]]\n",
      "Function output:\n",
      "[-0.15777734]\n",
      "Network output:\n",
      "[-0.15908463]\n",
      "\n",
      "Input:\n",
      "[[1.02127347]\n",
      " [1.36628162]]\n",
      "Function output:\n",
      "[0.33782151]\n",
      "Network output:\n",
      "[0.33667149]\n",
      "\n",
      "Input:\n",
      "[[1.56959945]\n",
      " [1.17881533]]\n",
      "Function output:\n",
      "[-0.2489706]\n",
      "Network output:\n",
      "[-0.24985876]\n",
      "\n",
      "Input:\n",
      "[[1.5]\n",
      " [1.7]]\n",
      "Function output:\n",
      "[0.13333333]\n",
      "Network output:\n",
      "[0.13230966]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the function that we are given\n",
    "def func(x):\n",
    "    y = (x[1]-x[0])/x[0]\n",
    "    return y\n",
    "\n",
    "print('Expected output for given function:')\n",
    "for i in range(6):\n",
    "    test_val = test_vals[:,i].reshape(2,1)\n",
    "    func_output = func(test_val)\n",
    "    net_output = calc_output(test_val)\n",
    "    print(f'Input:\\n{test_val}')\n",
    "    print(f'Function output:\\n{func_output}')\n",
    "    print(f'Network output:\\n{net_output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above ouput, we can see that the network does indeed approximate the given funciton. \n",
    "\n",
    "### 2a\n",
    "We are asked to write the explicit equations for the network given by the values below and draw its architecture, which is done by hand below. We are then asked to calculate the output and cost of the given training point and label, showing all intermediate steps. Doing this below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=np.array([[0.2,0.2],[-0.4,0.2],[0.1,-0.1]])\n",
    "b2=np.array([0.2,0.4,0.4]).reshape(3,1)\n",
    "w3=np.array([-0.1, 0.2, 0.2])\n",
    "b3=np.array([0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for [3;2]:\n",
      "z2:\n",
      "[[ 1.2]\n",
      " [-0.4]\n",
      " [ 0.5]]\n",
      "\n",
      "a2:\n",
      "[[0.76852478]\n",
      " [0.40131234]\n",
      " [0.62245933]]\n",
      "\n",
      "z3:\n",
      "[0.32790186]\n",
      "\n",
      "a3:\n",
      "[0.58124878]\n",
      "\n",
      "Cost: [0.08767629]\n"
     ]
    }
   ],
   "source": [
    "def calc_output(x, verbose=False):\n",
    "    z2 = w2@x+b2\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = w3@a2+b3\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    if verbose==True:\n",
    "        print(f'z2:\\n{z2}')\n",
    "        print(f'\\na2:\\n{a2}')\n",
    "        print(f'\\nz3:\\n{z3}')\n",
    "        print(f'\\na3:\\n{a3}')\n",
    "        return [z2,a2,z3,a3]\n",
    "        \n",
    "    return a3\n",
    "\n",
    "def calc_loss(output, label):\n",
    "    loss=0.5*np.square(label-output)\n",
    "    return loss\n",
    "\n",
    "\n",
    "print('Output for [3;2]:')\n",
    "x = np.array([[3],[2]])\n",
    "y = 1\n",
    "output = calc_output(x, verbose=True)\n",
    "loss=calc_loss(output[3], y)\n",
    "print(f'\\nCost: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b\n",
    "We are now asked to write out the backpropogation algorithm and use it to calculate the gradient of the cost function with respect to each set of weights $\\vec{w}^l$ and biases $\\vec{b}^l$. To do this, we start by finding the gradient of the output layer with respect the input values of the layer, which is given by $\\frac{\\partial C}{\\partial \\vec{z}^L}$. We will denote this quantity as $\\delta^L$, and use the chain rule to express this as \n",
    "$$\\delta^L=\\frac{\\partial C}{\\partial \\vec{z}^L}=\\frac{\\partial C}{\\partial \\vec{a}^L} \\frac{\\partial \\vec{a}^L}{\\partial \\vec{z}^L}$$\n",
    "\n",
    "Looking at the above equation, we see that $\\frac{\\partial C}{\\partial \\vec{a}^L}$ is easily calculated as $\\frac{d}{d\\vec{a}^L}(\\frac{1}{2}(y-a^L)^2)=y-a^L$, which we denote as $\\nabla_{\\vec{a}^L}C$. We can also see that $\\frac{\\partial \\vec{a}^L}{\\partial \\vec{z}^L}$ is simply the derivative of the activation function with respect to $z^L$, which is given by $\\frac{d}{dz^L}=(1+e^{-z^{L}})^{-1}=e^{-z^L}(1+e^{-z^L})^{-2}$, which we will denote as $\\sigma '(\\vec{z}^L)$. This gives us a very clean vector form of the above expression,\n",
    "$$\\vec{\\delta}^L=\\nabla_{\\vec{a}^L}C\\odot\\sigma '(\\vec{z}^L)$$\n",
    "\n",
    "\n",
    "We started with the last layer since we can calculate the derivative of the cost function with respect to the input of each neuron of the last layer directly. But what about earlier layers? For these, we need think about the effect of all layers after the layer we are adjusting as well, which makes the problem quite a bit harder. However, using the chain rule, we can show that the gradient of each layer can be expressed in terms of the the inputs of the current layer, the weights going to the next layer, and the $\\delta$ of the next layer, given by $\\vec{\\delta}^l=(W^{l+1})^T\\vec{\\delta^{l+1}}\\odot\\sigma '(\\vec{z}^l)$. Using this expression along with the one we derived for the output layer, we can build our way backward though each layer one at a time, hence the name backpropogation!\n",
    "\n",
    "However, we still need to know how to adjust our weights and biases for a given layer using $\\vec{\\delta}^l$. We can again use the chain rule, as is done in the course notes equation 7.13 and 7.14, to get \n",
    "$$\\nabla_{\\vec{b}^l}C=\\vec{\\delta}^l$$\n",
    "$$\\nabla_{W^l}C=\\vec{\\delta}^l(\\vec{a}^{l-1})^T$$\n",
    "\n",
    "Now we have explicit equations we need to calculate the gradient of the network with resect to all the weights and biases. Doing this below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_prime(x):\n",
    "    np.exp(-x)/np.square(1+np.exp(-x))\n",
    "\n",
    "def calc_grad(x, y, net_vals):\n",
    "    z2, a2, z3, a3 = net_vals[0], net_vals[1], net_vals[2], net_vals[3]\n",
    "    delta_3 = (y-a3)*sigma_prime(z3)\n",
    "    delta_2 = (w3.T@delta_3)*simga_prime(z2)\n",
    "    \n",
    "    # now calculate the gradients for weights and biases of layers 2 and 3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3]).reshape(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 0],\n",
       "       [-1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
